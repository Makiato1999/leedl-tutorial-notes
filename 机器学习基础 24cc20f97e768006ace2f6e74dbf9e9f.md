# 机器学习基础

梯度（gradient）本身指向的是损失函数增长最快的方向，而我们做梯度下降（Gradient Descent）是为了让损失变小，所以必须往反方向走，乘上 “−” 就变成下坡方向，所以参数会逐步走向损失最小的位置。

- 梯度正 → 上坡在正方向 → 往正方向走会让损失变大，我们往负方向走 → www 变小
- 梯度负 → 上坡在负方向 → 往负方向走会让损失变大，我们往正方向走 → www 变大

如果梯度 = 0，说明当前位置已经是平的（山坡坡度为 0）。无论学习率 η 多大，η×0=0，所以参数不再移动，训练自然停止。

- 但是可能到达了局部极小值（local minimum），也可能在平坦区（plateau）卡住。
- 实际深度学习训练时，第二种情况（梯度完全为 0）很少完全发生，因为数值上常常只是接近 0。所以会加一个**容忍阈值**（tolerance）。

Sigmoid 函数，如果 x1 的值，趋近于无穷大的时候，根据指数函数性质，e^−(b+wx1)会趋近于0，这一项就会消失，因此这一函数就会收敛在高度为 c 的地方。如果 x1 负的非常大的时候，e^−(b+wx1)会趋近于无穷，e^−(b+wx1)作为分母就会非常大，函数的值就会趋近于 0。

不同的w，改变斜率；不同的b，改变偏移；不同的c，改变高度。

![Screenshot 2025-08-15 at 11.05.29 PM.png](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2024cc20f97e768006ace2f6e74dbf9e9f/Screenshot_2025-08-15_at_11.05.29_PM.png)

Sigmoid 的数量是由自己决定的，而且 Sigmoid 的数量越多，可以产生出来的分段 线性函数就越复杂。Sigmoid 越多可以产生有越多段线的分段线性函数，可以逼近越复 杂的函数。Sigmoid 的数量也是一个超参数。

多个 Sigmoid 单元，是**同时**处理输入，而不是把 x 轴切成三段来各自负责。神经网络不是人为切段的，而是让模型自己学习每个 Sigmoid 单元的中心位置、斜率、偏移量。这样多个 Sigmoid 的组合（加权求和）就可以形成任意复杂的“折线”或“平滑曲线”。这个组合结果就是新的特征值，送到下一层去。

这个图只有一层隐藏层：输入 x → 线性变换 → 激活函数 → 加权求和 → 输出 y。没有第二层再继续激活的过程，所以它是一个单隐藏层网络。

![Screenshot 2025-08-12 at 12.26.25 AM.png](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2024cc20f97e768006ace2f6e74dbf9e9f/Screenshot_2025-08-12_at_12.26.25_AM.png)

定义损失，损失是一个函数，其输入就是一组参数，去判断这一组参数的好坏

θ 是整个模型的全部参数向量（比如神经网络里的所有 w 和 b 打包在一起）。上标 0 / 1 / 2 / 3 代表的是迭代轮次。

batch是指你训练集的一小部分数据。

在mini-batch 梯度下降里，更新是串行的，不是并行。每次梯度计算都只用一个 batch 的数据，每个 batch 的梯度更新都会影响下一批的参数，直到用完一个 epoch 的所有 batch。

把所有的批量都看过一次，称为一个回合(epoch)，每一次更新参数叫做一次更新。

![Screenshot 2025-08-12 at 1.58.05 AM.png](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2024cc20f97e768006ace2f6e74dbf9e9f/Screenshot_2025-08-12_at_1.58.05_AM.png)

2 个 ReLU 才能够合成一个 Hard Sigmoid

![Screenshot 2025-08-12 at 2.06.22 AM.png](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2024cc20f97e768006ace2f6e74dbf9e9f/Screenshot_2025-08-12_at_2.06.22_AM.png)

这个图描述的是一个多层神经网络（MLP）的前向计算。第一层：输入 x → 线性变换 → 激活函数（σ 或 ReLU） → 得到隐藏层输出 a。第二层：隐藏层输出 a → 线性变换 → 激活函数 → 得到新的输出 a′（可能是下一层输入，或者直接是预测结果）

多层网络的本质：每一层都是“线性变换 + 激活函数”，然后把输出传给下一层。

![Screenshot 2025-08-12 at 2.20.02 AM.png](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2024cc20f97e768006ace2f6e74dbf9e9f/Screenshot_2025-08-12_at_2.20.02_AM.png)

Sigmoid 或 ReLU 称为神经元(neuron)，很多的神经元称为神经网络 (neural network)。每一排称为一层，称为隐藏层(hidden layer)，很多的隐藏层就“深”，这套技术称为深度学习。

深度学习的训练会用 到反向传播(BackPropagation，BP)，其实它就是比较有效率、算梯度的方法。

在训练数据和测试 数据上的结果是不一致的，这种情况称为过拟合(overfitting)。

![Screenshot 2025-08-12 at 2.23.01 AM.png](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2024cc20f97e768006ace2f6e74dbf9e9f/Screenshot_2025-08-12_at_2.23.01_AM.png)

# 机器学习的整体框架

先写出一个有未知数 θ 的函数 fθ (x)

- θ 是**参数集合**（不是一个数，而是一堆权重和偏置等参数），比如：全连接层里的 W（权重矩阵）、b（偏置）、甚至卷积核参数等。
- 模型就是一个“函数”，输入x，输出预测值y^。
- 激活函数（sigmoid、ReLU 等）在这里是模型内部的计算环节，用来增加非线性能力，让模型学到更复杂的映射关系。

定义损失函数 L(θ)

- 损失函数（Loss Function）不是激活函数，它的作用是衡量预测值和真实值之间的差异。
- 常见的损失函数：回归任务：均方误差（MSE）、分类任务：交叉熵（Cross-Entropy）。
- “梯度”“偏导”是在优化阶段用的，损失函数本身就是一个普通的数学函数，它对 θ 可导，这样我们才能用梯度下降更新参数。

![Screenshot 2025-08-12 at 2.39.48 AM.png](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2024cc20f97e768006ace2f6e74dbf9e9f/Screenshot_2025-08-12_at_2.39.48_AM.png)

解优化问题 → 找到最优参数 θ∗

- 找一个 θ，该 θ 可以让损失的值越小越好。
- 优化方法：梯度下降（Gradient Descent），其中 η 是学习率（learning rate）。
- 反向传播（Backpropagation）就是用链式法则计算每个参数的梯度。

![Screenshot 2025-08-12 at 2.39.13 AM.png](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2024cc20f97e768006ace2f6e74dbf9e9f/Screenshot_2025-08-12_at_2.39.13_AM.png)

# 机器学习的流程

如果上面的整体框架还是没梳理清楚，看这个流程的分解

1. 首先，正向传播（Forward Propagation）
    1. 输入数据 x，也就是特征向量
    2. 隐藏层计算，每一层都会先做线性组合，z^[l] = W^[l] a^[l−1] + b^[l]
    3. 再通过激活函数得到非线性输出，a^[l] = σ(z^[l])
        
        隐藏层输出 a^[l] 不是等于预测的 y，它只是“中间特征表示”。只有最后一层的 a^[L] 才是 y^
        
    4. 如果到了最后一层（第 L 层）：输出 y^（预测结果）

总结，第1层得到a^[1]，再传给第2层计算 a^[2]，直到第 L 层，得 y^

1. 损失函数（Loss Function）
    
    这个部分和上面整体框架里聊的是一样的，在这里只是展示损失函数是在有 y^之后才使用的
    

总结，所以损失函数是在“整个正向传播结束”之后才算的

1. 反向传播（Backpropagation）
    1. 目标：算出损失函数对每一层参数的梯度。
    2. 利用链式法则逐层回传（用偏导数），∂L / ∂W^[l], ∂L / ∂b^[l]
    3. 反向传播的时候，你需要“把正向传播过程中算的每一层的 z^[l]，a^[l] 都拿出来”，因为梯度公式依赖它们

1. 参数更新（Parameter Update）
    1. 有了梯度以后，用优化算法更新参数：
    2. 更新完以后，就进入下一次迭代（下一次正向传播）

![Screenshot 2025-08-16 at 2.26.52 AM.png](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2024cc20f97e768006ace2f6e74dbf9e9f/Screenshot_2025-08-16_at_2.26.52_AM.png)

总结

训练就是 不断迭代前向传播 + 反向传播：

1. 前向传播：输入 x → 多层线性 + 激活 → 预测 y^。
2. 计算损失：用损失函数衡量 y^ 和真实 y 的差距。
3. 反向传播：根据损失对参数 θ 求梯度。
4. 参数更新：用梯度下降（SGD/Adam等优化器）更新参数。