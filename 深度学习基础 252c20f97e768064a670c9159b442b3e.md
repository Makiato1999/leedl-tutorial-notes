# 深度学习基础

区别局部极小值 local minimum vs 鞍点 saddle point

- 局部极小值：真没路走了，已经是最低点（只能接受）。
- 鞍点：还可以走，只是某些方向看起来“平坦”或“误导”，导致梯度下降方法停在那。

红点所在位置像“马鞍”：

沿 x 方向看，它是一个低点

沿 y 方向看，它却是一个高点

特征：梯度为 0，但不是最低点

模型训练可能在这里卡住，因为梯度下降法只依赖梯度信息 → 梯度接近 0 时会停滞

![Screenshot 2025-08-17 at 4.54.36 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-17_at_4.54.36_PM.png)

### 如何通过数学方法来判断一个临界点（梯度为 0）到底是局部极小值、局部极大值还是鞍点？

用泰勒展开近似损失函数，我们把损失函数在 θ′ 附近展开：

- 第一项：常数 L(θ′)
- 第二项：和 梯度 g 有关（一阶项，表示坡度）。
- 第三项：和 Hessian 矩阵 H 有关（二阶项，表示弯曲程度）。

Hessian 里的元素是“二阶偏导数”，因为函数 f(x,y) 有多个变量，每个方向都可以分别求导。

Hessian > 0 → 碗口向上 → 谷底（极小值）

Hessian < 0 → 碗口向下 → 山顶（极大值）

Hessian 有正有负 → 马鞍形 → 鞍点

![Screenshot 2025-08-17 at 5.42.10 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-17_at_5.42.10_PM.png)

![Screenshot 2025-08-17 at 6.59.23 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-17_at_6.59.23_PM.png)

![Screenshot 2025-08-17 at 6.59.48 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-17_at_6.59.48_PM.png)

对称矩阵的特性：一定可以被正交对角化，特征值一定都是实数。

特征向量：不会被矩阵旋转的方向。

特征值：矩阵在这个方向上的拉伸倍数。

在 Hessian 场景里，特征值的符号 = 弯曲方向。

Hessian 的关键特性是“对称”，这保证了它的特征值能真实反映函数的曲率。通过特征值的符号，就能判断临界点是极小值、极大值还是鞍点。

书上的例子（3.6）

![Screenshot 2025-08-17 at 9.15.17 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-17_at_9.15.17_PM.png)

![Screenshot 2025-08-17 at 9.15.28 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-17_at_9.15.28_PM.png)

用行列式 det(H−λI) 计算特征值 

det(H−λI) = (−λ)(−λ)−(−2)(−2) = λ^2−4

λ^2−4 = 0 ⇒ λ = ±2

![Screenshot 2025-08-17 at 9.17.23 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-17_at_9.17.23_PM.png)

实际上，我们几乎不会真的把海森矩 阵算出来，因为海森矩阵需要算二次微分，计算这个矩阵的运算量非常大，还要把它的特征值 跟特征向量找出来，所以几乎没有人用这个方法来逃离鞍点

深度学习里局部极小值不太可怕

- 神经网络动辄有 百万甚至上亿个参数 → 误差表面是一个 高维空间。
- 在高维空间里，真正“封闭”的局部极小值很少，大多数点只是 鞍点：在某些方向上曲率是正的（像谷底），在另一些方向上曲率是负的（像山脊），所以还可以继续下降

# 批量和动量

随机梯度下降 (Stochastic Gradient Descent, SGD)，小批量梯度下降 (Mini-batch Gradient Descent)，严格来说：“SGD”这个词在论文/代码里经常默认就是 mini-batch SGD（而不是单样本），只是历史命名沿用下来而已。

### 小批量梯度下降（mini-batch gradient descent） 和 k 折交叉验证（k-fold cross validation） 完全不是一回事!

小批量梯度下降

- 这是 **训练时的一种优化算法**
- 目的：让参数更新既快（不像全量那么慢），又稳（不像单样本那么抖）
- 做法：把训练数据分成一批一批的（batch），比如 batch size = 32，训练的时候每次只用 32 条数据算梯度，更新一次参数
    
    👉 它解决的是 怎么更新参数 的问题
    

k 折交叉验证 

- 这是 **模型评估方法**
- 目的：在数据有限的情况下，可靠地评估模型的泛化能力
- 做法：把数据切成 k 份，每次拿 k-1 份做训练，剩下 1 份做验证；重复 k 次，最后取平均分数
    
    👉 它解决的是 怎么评估模型 的问题
    

batch 的组成方式，随机划分 (shuffle)（实践中最常用）：

- 每个 epoch 开始时，把 1000 条数据随机打乱
- 然后再分成 10 个 batch（每个 batch 100 条）
- 这样第 1 个 batch 可能是 {1,15,17,120,...}，第 2 个 batch 就是剩下的另一组随机的 100 条
- 每个 epoch shuffle 一次，所以每次训练的 batch 顺序和内容都会变

顺序更新 θ 是什么意思？

在训练中，batch 的处理是串行的：

1. 第 1 个 batch：
    - 用当前参数 θ0 计算 loss → 得到梯度 ∇L1(θ0)
    - 更新参数：θ1 = θ0 − η ∇L1(θ0)
2. 第 2 个 batch：
    - 用刚更新过的参数 θ1，在第 2 个 batch 上算 loss → 得到梯度 ∇L2(θ1)
    - 更新参数：θ2 = θ1 − η ∇L2(θ1)
3. 第 3 个 batch：
    - 用参数 θ2，在第 3 个 batch 上算 → 更新到 θ3
    
    …
    
    直到 10 个 batch 都走完，你得到 θ10，这就是一个 epoch
    

总结

参数更新是 线性的流程：batch1 → batch2 → batch3 → …

如果 batch1 的梯度=0 → 这次没更新，参数照样传给 batch2。

batch2 再算的时候，多半能算出非零梯度，于是参数就会继续走。

这就是所谓的 mini-batch 自带“抖动”，不会像全批量那样轻易被卡死。

小批量梯度下降：蓝点落在比较宽、平缓的谷底。更新更慢，但能找到更好、泛化能力更强的解。测试损失也不会差太多（黑实线训练损失 vs 黑虚线测试损失）。

大批量梯度下降：蓝点落在一个又窄又深的谷底。更新更快，但容易过拟合，测试表现较差。换到测试集，损失立刻差很多（红色虚线很长）。

![Screenshot 2025-08-18 at 5.55.07 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-18_at_5.55.07_PM.png)

![Screenshot 2025-08-18 at 6.02.18 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-18_at_6.02.18_PM.png)

动量法 (momentum method)

普通梯度下降：历史影响被“压缩”进了位置 θt，所以每次只用当前位置 + 当前梯度来更新。

动量法：历史影响被显式地保留在 之前所有梯度的加权和 mt 里，每次更新都会把“过去的趋势”带上。

即使在某个点梯度为 0：

由于前一步还“有惯性”（蓝色虚线箭头），真实的更新方向（蓝色实线箭头）依然 继续往前走。

这就让优化器 不容易卡住，能够冲过一些小坑或鞍点，继续往更好的极小值方向走。

![Screenshot 2025-08-18 at 6.31.22 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-18_at_6.31.22_PM.png)

![Screenshot 2025-08-18 at 6.35.30 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-18_at_6.35.30_PM.png)

# 自适应学习率

学习率大，学习率 η = 10^−2 的结果如图 3.22(a) 所示。参数在峡谷的两端，参数在山壁的两端不断 地“震荡”，损失降不下去，但是梯度仍然是很大的。

学习率小，调到 10^−7 的结果如图 3.22(b) 所示，参数不再“震荡”了。参数会滑到山谷底后左 转，但是这个训练永远走不到终点。

黄色叉号就是最终想要达到的 最优参数位置

![Screenshot 2025-08-18 at 7.26.45 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-18_at_7.26.45_PM.png)

所以所有的参数都是设同样的学习率，这显然是不够，引入自适应学习率方法

梯度比较大的时候，学习率就减小，梯度比较小的时候，学习率就放大。

![Screenshot 2025-08-18 at 7.34.58 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-18_at_7.34.58_PM.png)

AdaGrad(Adaptive Gradient)

![Screenshot 2025-08-18 at 7.54.57 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-18_at_7.54.57_PM.png)

学习率从 η 改成 η / σ 的时候，学习率就变得参数相关(parameter dependent)，常见的类型是算梯度的均方根(root mean square) σ

- 因为分母 σ 把过去所有梯度的“历史信息”带进来了
- 如果某个参数的梯度经常很大，分母越来越大，该参数更新得越来越慢；
- 如果某个参数的梯度经常很小，分母增长慢，该参数还能继续更新。

![Screenshot 2025-08-18 at 7.53.29 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-18_at_7.53.29_PM.png)

RMSprop(Root Mean Squared propagation)

![Screenshot 2025-08-18 at 8.08.24 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-18_at_8.08.24_PM.png)

计算 θ 的方法跟 AdaGrad 算均方根不一样，AdaGrad 每一个梯度有同等的重要性

- 和 AdaGrad 不同，RMSProp 不会把所有历史梯度都平均，而是给“最近的梯度”更大权重，逐渐“遗忘”很久之前的梯度。
- α 是遗忘因子 (通常取 0.9 左右)。
- 好处：避免了学习率不断衰减到接近 0。

![Screenshot 2025-08-18 at 8.01.09 PM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-18_at_8.01.09_PM.png)

总结：

- SGD：最基础，每次用小批量数据更新参数，更新全靠梯度。
- Momentum：避免“抖动”，参数更新时考虑“惯性”，像小球带惯性往下滚。
- RMSProp：每个参数有独立学习率，梯度平方的指数加权平均，自动调整学习率，控制大梯度更新小点，小梯度更新大点。
- Adam：Momentum + RMSProp 结合，使用动量作为参数更新方向，并且能够自适应调整学习率。常用的优化的策略或者优化器(optimizer)是 Adam (Adaptive moment estimation)，PyTorch 已经预设。

### 学习率调度

AdaGrad 存在问题：分母 σ 累计了历史上所有梯度的平方 → 越到后面，它越大。有效学习率 η/σ会越来越小

学习率调度(learning rate scheduling)可以解决这个问题

- 在 BC 上走了很久，w 方向的梯度一直很小 → σ^w 始终很小 → η/σ^w 变得很大（对 w 的有效学习率被“放大”）。
- 一到 C 附近，哪怕出现 一点点非零的 g^w，因为分母很小，更新量 | Δw | 会一下子变得很大 → 轨迹就出现了竖直的大跳（看起来像“爆炸”）。
- 跳了几次后，w 方向的梯度变大被累积进 σ^w → 分母变大 → 步长变小 → 轨迹又稳定并回到正确路线。

![Screenshot 2025-08-19 at 1.49.26 AM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-19_at_1.49.26_AM.png)

![Screenshot 2025-08-19 at 1.21.02 AM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-19_at_1.21.02_AM.png)

调度策略：

1. 学习率衰减(learning rate decay)，也称为学习率退火(learning rate annealing)
    - 加了学习率调度，η 在后期越来越小，即使分母 σ 小，也能抵消掉分母带来的影响，步伐稳定、平顺，最后顺利收敛。防止它在红圈处突然“爆炸”。
    - 到达指定 step 就乘以一个因子（如 0.1），大步探索，逐渐减小步伐，收敛更稳定。
2. Learning Rate Warmup（学习率预热），它在深度学习里非常常见，特别是在 ResNet、BERT、Transformer 这类大模型的训练中
    - 刚开始训练时，让学习率先变大后变小，至于变到多大、变大的速度、变小的速度是超参数。避免在参数随机、统计量不稳定时用大学习率导致“乱跳”。

# 优化总结

这个是目前优化的完整的版本，这个版本里面有动量，其不是顺着某个时刻算出的梯度方向来更新参数，而是把过去所有算出梯度的方向做一个加权总和当作更新的方向。

步伐大小为 m / σ 。最后通过 η 来实现学习率调度

![Screenshot 2025-08-19 at 3.09.42 AM.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20252c20f97e768064a670c9159b442b3e/Screenshot_2025-08-19_at_3.09.42_AM.png)

动量 m：方向性累积（正负能抵消）。负责“朝哪个方向走 + 走多快”。

均方根 σ：能量累积（平方 → 全是正数，没法抵消）。负责“控制尺度，别走得太猛”。

动量 → 平滑方向。均方根 → 控制步长。

结合起来 → Adam 的核心思想 ✅

# 分类

为什么分类过程中要加上 softmax 函数?