# 实践方法论

模型偏差，优化问题

增加模型的灵活性

简单模型 → 容易学不到位（偏差大）；

更复杂的模型（多特征、多层神经网络） → 表达能力更强，更容易学到接近真实规律的结果

![Screenshot 2025-08-16 at 2.42.47 AM.png](%E5%AE%9E%E8%B7%B5%E6%96%B9%E6%B3%95%E8%AE%BA%2024dc20f97e7680ca9b06c333d672a42e/Screenshot_2025-08-16_at_2.42.47_AM.png)

并不是所有的结果不好，都叫做过拟合！

深层神经网络比浅层网络更有表达能力，理论上能表示更复杂的函数。按理说，深模型的损失应该比浅模型更低。

- 如果你发现：深模型的损失并没有比浅模型更低，那说明问题不在“模型能力”，而在优化没做好
- 常见原因：梯度消失、梯度爆炸、学习率设置不合理、网络初始化不好等

判断与应对流程

1. 先看训练损失
    - 如果训练损失大 → 判断是 **模型偏差** 还是 **优化问题**：
        - 损失随模型增大（特征更多，网络更深/更宽，更复杂的函数族）始终很高 → 偏差问题 → 增大模型容量。
        - 损失本来可以很低，结果变高（同一类模型里，小模型能把训练损失压得很低，但大模型反而没有更好，甚至更差）→ 优化问题 → 改优化方法。
2. 再看测试损失
    - 如果训练损失小，测试损失也小 → ✅ 模型已经足够好，训练结束。
    - 如果训练损失小，测试损失大 → ❌ 过拟合 → 需要正则化、更多数据、早停等方法。

总结

偏差问题：模型再怎么加复杂度（加层、加特征），训练损失还是高 → 说明模型本身太弱。

优化问题：已有小模型能把训练损失降很低，但更复杂的模型反而没降下去，甚至更差 → 说明训练过程出了问题（优化器、梯度、初始化等）。

# 过拟合

模型在训练集上表现很好，但在测试集上损失反而更大

模型太灵活 → 容易记住训练集里的噪声 → 测试集上就会犯错

模型复杂度太低 → 欠拟合；

模型复杂度太高 → 过拟合；

最佳模型 = 让测试损失最低的那个复杂度

不能只看训练损失，而要综合考虑 测试损失。

![Screenshot 2025-08-17 at 3.06.13 AM.png](%E5%AE%9E%E8%B7%B5%E6%96%B9%E6%B3%95%E8%AE%BA%2024dc20f97e7680ca9b06c333d672a42e/Screenshot_2025-08-17_at_3.06.13_AM.png)

# 交叉验证

把数据分成两部分：**训练集 (training set)**：比如 90%，用来训练模型参数。**验证集 (validation set)**：比如 10%，用来评估不同模型的效果。

风险：过度依赖公开测试集

如果反复根据公开测试集的分数调整模型，就会对公开集过拟合，避免“把测试集当成调参工具”，从而降低过拟合到测试集的风险！

验证集划分

k 折交叉验证 (k-fold cross validation)

- 把训练集切成 k 份
- 每次拿 1 份做验证，剩下 k-1 份做训练
- 轮流进行 k 次，取平均分数作为模型效果

![Screenshot 2025-08-17 at 3.42.05 AM.png](%E5%AE%9E%E8%B7%B5%E6%96%B9%E6%B3%95%E8%AE%BA%2024dc20f97e7680ca9b06c333d672a42e/Screenshot_2025-08-17_at_3.42.05_AM.png)

不匹配